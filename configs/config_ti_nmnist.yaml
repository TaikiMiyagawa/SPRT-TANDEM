# Path to tensorboard logs: `root_dir`/'root_tblogs'/'subproject_name'_'exp_phase'/'comment'_'time_stamp'/eventsXXX
# Path to database files: `root_dir`/'root_dblogs'/'subproject_name'_'exp_phase'.db
# Path to checkpoint files: `root_dir`/'root_ckptlogs'/'subproject_name'_'exp_phase'/'comment'_'time_stamp'/ckptXXX
root_dir: "~"
project_name: "SPRT-TANDEM"
name_dataset: "nosaic_mnist"
root_tblogs: "/data-directory/SPRT-TANDEM/nosaic_mnist/tblogs" # where TensorBoard logs will be saved
root_dblogs: "/data-directory/SPRT-TANDEM/nosaic_mnist/dblogs" # where .db files generated by optuna will be saved
root_ckptlogs: "/data-directory/SPRT-TANDEM/nosaic_mnist/ckptlogs" # # where checkpoint files will be saved
tfr_train: "/data-directory/nosaic_mnist/nosaic_mnist_feat_train.tfrecords" 
    # tfr_train is to be split to training and validation datasets
    # This scripts assumes that LSMT's input is the features extracted by the feature extractor NN.
    # Thus the feat_dim is not 784 (orignal NMNIST), but 128.
tfr_test: "/data-directory/nosaic_mnist/nosaic_mnist_feat_test.tfrecords"
num_traindata: 50000 # num of training videos
num_validdata: 10000 # num of validation videos
num_testdata: 10000 # num of test videos
duration: 20 # num of frames in a single video
feat_dim: 128 # feature dimension


# Training config
gpu: 0 # Whic GPU is used
subproject_name: "1st_order_SPRT"  # used for names of directories and log files
comment: "_" # used for TensorBoard logs
nb_trials: 1 # how many trials for hparameter tuning
exp_phase: "try" # try: one-shot, tuning: optuning, stat: to take statistics 

flag_resume: False # Whether to restore a model or not
path_resume: "/data-directory/SPRT-TANDEM/nosaic_mnist/ckptlogs/..." 
    # Path to directory in which ckpt files are saved
    # This is ignored if flag_resume = False
flag_seed: False
    # Whether to fix numpy and tensorflow seeds
seed: 7
    # is ignored if flag_seed is False
train_display_step: 20 # Outputs stdout log per this steps
valid_step: 50 # validation per 
max_to_keep: 3 # latest `max_to_keep` checkpoints will be saved

# Model 
order_sprt: 0 # Order of the N-th Markovian approximation of the SPRT
learning_rates: [1e-2, 1e-3] # learning rate decay is available
decay_steps: [100000000000,] # learning rate is decayed at this step
weight_decay: 0.00001
name_optimizer: "adam" # adam, rmsprop, sgd, momentum, ... See optimizers.py

param_multiplet_loss: 0. # prefactor of the multiplet loss in the total loss
param_llr_loss: 1.  # prefactor of the LLLR in the total loss
dropout: 0. # 0 is no Dropout
width_lstm: 128 # num of hidden states in the LSTM
activation: "tanh" # sigmoid, linear, ... Default is "tanh"
flag_wd: True # Use weight decay or not

# Other minor hyperparameters
nb_epochs: 100 
batch_size: 1000 # =1 is not currently supported
list_alpha: [1e-3, 1e-7] # Target FPRs, to be used to generate SPRT's thresholds in validation
list_beta: [1e-3, 1e-7] # Target FNRs, to be used to generate SPRT's thresholds in validation

# Data properties
nb_cls: 2 # Num of classes

# Search space for optuna
list_lr: [1e-2, 1e-3, 1e-4, 1e-5] # learning rate
list_bs: [256, 512, 1024] # batch size
list_opt: ["adam", "momentum", "adagrad", "rmsprop"] # optimizer
list_do: [0., 0.1, 0.2, 0.3, 0.4, 0.5] # Dropout
list_wd: [0.001, 0.0001, 0.00001] # weight decay