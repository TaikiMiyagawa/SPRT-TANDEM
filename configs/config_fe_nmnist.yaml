""" See `config_ti_nmnist.yaml` for more descriptions of the parameters."""
# Path to tensorboard logs: `root_dir`/'root_tblogs'/'subproject_name'_'exp_phase'/'comment'_'time_stamp'/eventsXXX
# Path to database files: `root_dir`/'root_dblogs'/'subproject_name'_'exp_phase'.db
# Path to checkpoint files: `root_dir`/'root_ckptlogs'/'subproject_name'_'exp_phase'/'comment'_'time_stamp'/ckptXXX
root_dir: "~"
project_name: "SPRT-TANDEM"
name_dataset: "nosaic_mnist"
root_tblogs: "/data-directory/SPRT-TANDEM/nosaic_mnist/tblogs" # where TensorBoard logs will be saved
root_dblogs: "/data-directory/SPRT-TANDEM/nosaic_mnist/dblogs" # where .db files generated by optuna will be saved
root_ckptlogs: "/data-directory/SPRT-TANDEM/nosaic_mnist/ckptlogs" # # where checkpoint files will be saved
tfr_train: "/data-directory/nosaic_mnist/nosaic_mnist_train.tfrecords" 
    # tfr_train is to be split to training and validation datasets
tfr_test: "/data-directory/nosaic_mnist/nosaic_mnist_test.tfrecords"
num_traindata: 50000
num_validdata: 10000
num_testdata: 10000
duration: 20
feat_dims: [28, 28, 1]


# Training config
gpu: 0
subproject_name: "feature_extraction"
comment: "_"
nb_trials: 1
exp_phase: "try"

flag_resume: False
path_resume: "/data-directory/SPRT-TANDEM/nosaic_mnist/ckptlogs/..." 
flag_seed: False
seed: 7
train_display_step: 20
valid_step: 200
max_to_keep: 3

# Model
name_optimizer: "rmsprop" 

resnet_size: 110 # ResNet num of layers
final_size: 128 # Num of final channels after the GAP layer
flag_wd: True 
resnet_version: 1 
    # See the ResNet's original paper.
    # V1 is for low resolution datasets (like CIFAR, MNIST).
    # V2 is for high resolution datasets (like ImageNet).

    # Reference:
    # ResNet v1
    # 8  : num residual blocks = [1, 1, 1], # num params = 0.09M, channel = (16, 32, 64)
    # 14 : [2, 2, 2],    # 0.18M
    # 20 : [3, 3, 3],    # 0.25M
    # 32 : [5, 5, 5],    # 0.46M
    # 44 : [7, 7, 7],    # 0.66M
    # 56 : [9, 9, 9],    # 0.85M
    # 110: [18, 18, 18], # 1.7M
    # 218: [36, 36, 36]  # 3.4M

# Hyperparameters
nb_epochs: 10
batch_size: 64 # =1 is not currently supported 
learning_rates: [1e-3, 1e-4, 1e-5]
decay_steps: [1500000000, 3000000000] 
weight_decay: 0.0001 

# Data properties
nb_cls: 2

# Search space for optuna
list_lr: [1e-2, 1e-3]
list_bs: [64]
list_opt: ["adam", "momentum", "rmsprop"]
list_do: [0.]
list_wd: [0.001, 0.0001, 0.00001]

""" See `config_ti_nmnist.yaml` for more descriptions of the parameters."""